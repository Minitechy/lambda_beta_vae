{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc8ada-e739-4fdf-b644-bc37f2532a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from scipy.linalg import inv\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.patches import Rectangle\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Configure threading for numerical libraries\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for experiment parameters.\"\"\"\n",
    "    EPSILON = 5e-5\n",
    "    NUM_SOLS = 50\n",
    "    BETA_ARR = np.array([1, 4, 8, 16, 32])\n",
    "    LAMBDA_ARR = np.array([0, 2, 4, 6, 8])\n",
    "    N_VALUES = [50, 100]\n",
    "    MS_MAP = {50: (10, 5), 100: (10, 5)}\n",
    "    MAX_ITERS_MAP = {\n",
    "        'fixed_point': {50: 50000, 100: 100000},\n",
    "        'adamw': {50: 30000, 100: 50000}\n",
    "    }\n",
    "    TOL_ERR_MAP = {50: 1e-6, 100: 1e-5}\n",
    "    GRAD_NORM_MAP = {\n",
    "        'adamw': {50: {'AB': 1.0, 'LZ_LW': 2.0}, 100: {'AB': 2.0, 'LZ_LW': 3.0}}\n",
    "    }\n",
    "    SAVE_PATH = os.path.join(os.getcwd(), \"linear_betavae_results\")\n",
    "    METHODS = ['fixed_point', 'adamw']\n",
    "    \n",
    "CONFIG = Config()\n",
    "os.makedirs(CONFIG.SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "def generate_diag_cov(s, min_var=0.1, max_var=1.0):\n",
    "    \"\"\"Generate diagonal covariance matrix with uniform random variances.\"\"\"\n",
    "    return np.diag(np.random.uniform(min_var, max_var, s))\n",
    "    \n",
    "def initialize_inputs(n, m, method, seed=None):\n",
    "    \"\"\"Generate initial matrices for optimization.\"\"\"\n",
    "    if method == 'fixed_point':\n",
    "        B = np.random.rand(m, n)\n",
    "        Sigma_W = make_spd_matrix(m)\n",
    "        return np.concatenate((B.ravel(), Sigma_W.ravel()))\n",
    "    else:\n",
    "        torch.manual_seed(seed)\n",
    "        A = torch.randn(n, m, dtype=torch.float64)\n",
    "        B = torch.randn(m, n, dtype=torch.float64)\n",
    "        Sigma_Z = torch.tensor(make_spd_matrix(n), dtype=torch.float64)\n",
    "        L_Z = torch.linalg.cholesky(Sigma_Z)\n",
    "        Sigma_W = torch.tensor(make_spd_matrix(m), dtype=torch.float64)\n",
    "        L_W = torch.linalg.cholesky(Sigma_W)\n",
    "        return np.concatenate((A.numpy().ravel(), B.numpy().ravel(), L_Z.numpy().ravel(), L_W.numpy().ravel()))\n",
    "        \n",
    "# =========================\n",
    "# Loss Computations\n",
    "# =========================\n",
    "def compute_recon_err(A, B, Sigma_Z, Sigma_W, n, Sigma_Y, is_torch=False):\n",
    "    \"\"\"Compute negative log-likelihood reconstruction error.\"\"\"\n",
    "    Sigma_X = B @ Sigma_Y @ B.T + Sigma_W\n",
    "    inv_Sigma_Z = torch.inverse(Sigma_Z) if is_torch else inv(Sigma_Z)\n",
    "    pi_term = torch.tensor(2 * np.pi, dtype=torch.float64) if is_torch else 2 * np.pi\n",
    "    det_func = torch.det if is_torch else np.linalg.det\n",
    "    trace_func = torch.trace if is_torch else np.trace\n",
    "    return -0.5 * (\n",
    "        trace_func(A.T @ inv_Sigma_Z @ Sigma_Y @ B.T) +\n",
    "        trace_func(inv_Sigma_Z @ A @ B @ Sigma_Y) -\n",
    "        trace_func(inv_Sigma_Z @ Sigma_Y) -\n",
    "        trace_func(A.T @ inv_Sigma_Z @ A @ Sigma_X) -\n",
    "        n * (torch.log(pi_term) if is_torch else np.log(pi_term)) -\n",
    "        (torch.log(det_func(Sigma_Z)) if is_torch else np.log(det_func(Sigma_Z)))\n",
    "    )\n",
    "    \n",
    "def compute_kl(Sigma_X, Sigma_W, m, is_torch=False):\n",
    "    \"\"\"Compute KL-divergence term.\"\"\"\n",
    "    det_func = torch.det if is_torch else np.linalg.det\n",
    "    trace_func = torch.trace if is_torch else np.trace\n",
    "    log_term = torch.log(det_func(Sigma_W)) if is_torch else np.log(det_func(Sigma_W))\n",
    "    return 0.5 * (trace_func(Sigma_X) - log_term - m)\n",
    "    \n",
    "def compute_l2_norm_loss(A, B, Sigma_Y, Sigma_W, n, is_torch=False):\n",
    "    \"\"\"Compute L2-norm regularization loss.\"\"\"\n",
    "    eye_n = torch.eye(n, dtype=torch.float64) if is_torch else np.eye(n)\n",
    "    trace_func = torch.trace if is_torch else np.trace\n",
    "    return trace_func((eye_n - A @ B) @ Sigma_Y @ (eye_n - A @ B).T + A @ Sigma_W @ A.T)\n",
    "    \n",
    "def compute_vae_loss(A, B, Sigma_Z, Sigma_W, beta, n, m, Sigma_Y, lambda_=0, is_torch=False):\n",
    "    \"\"\"Compute VAE loss with optional λ regularization.\"\"\"\n",
    "    Sigma_X = B @ Sigma_Y @ B.T + Sigma_W\n",
    "    recon_err = compute_recon_err(A, B, Sigma_Z, Sigma_W, n, Sigma_Y, is_torch)\n",
    "    kl = compute_kl(Sigma_X, Sigma_W, m, is_torch)\n",
    "    beta_loss = recon_err + (torch.tensor(beta, dtype=torch.float64) if is_torch else beta) * kl\n",
    "    total_loss = beta_loss\n",
    "    if lambda_ != 0:\n",
    "        l2_norm_loss = compute_l2_norm_loss(A, B, Sigma_Y, Sigma_W, n, is_torch)\n",
    "        total_loss += (torch.tensor(lambda_, dtype=torch.float64) if is_torch else lambda_) * l2_norm_loss\n",
    "    return recon_err, total_loss\n",
    "    \n",
    "# =========================\n",
    "# Metrics\n",
    "# =========================\n",
    "def compute_SAP(cov_XV, m, s):\n",
    "    \"\"\"Compute SAP score for disentanglement evaluation.\"\"\"\n",
    "    cov_XV_block = cov_XV[:m, m:m+s]\n",
    "    variances_X = np.diag(cov_XV)[:m, None]\n",
    "    variances_V = np.diag(cov_XV)[m:m+s, None].T\n",
    "    squared_correlations = (cov_XV_block ** 2) / (variances_X @ variances_V)\n",
    "    top_indices = np.argsort(squared_correlations, axis=0)[-2:][::-1]\n",
    "    sap_score = np.mean(\n",
    "        squared_correlations[top_indices[0], np.arange(s)] -\n",
    "        squared_correlations[top_indices[1], np.arange(s)]\n",
    "    )\n",
    "    return sap_score if not np.isnan(sap_score) else np.nan, squared_correlations\n",
    "    \n",
    "def compute_Im(cov_XV, m, s):\n",
    "    \"\"\"Compute maximum mutual information sum using Hungarian algorithm.\"\"\"\n",
    "    cov_XV_block = cov_XV[:m, m:m+s]\n",
    "    variances_X = np.diag(cov_XV)[:m, None]\n",
    "    variances_V = np.diag(cov_XV)[m:m+s, None].T\n",
    "    rho_ij_sq = (cov_XV_block ** 2) / (variances_X @ variances_V)\n",
    "    mi_XV = -0.5 * np.log(1 - rho_ij_sq)\n",
    "    cost_matrix = -mi_XV\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    mi_max = -cost_matrix[row_ind, col_ind].sum()\n",
    "    return mi_max if not np.isnan(mi_max) else np.nan, mi_XV\n",
    "    \n",
    "# =========================\n",
    "# Optimization\n",
    "# =========================\n",
    "def optimize(inputs, beta, lambda_, n, m, s, Sigma_Y, Gamma, Sigma_V, method, tol_err=None, inv_Sigma_Y=None):\n",
    "    \"\"\"Perform optimization for β-VAE or λβ-VAE using specified method.\"\"\"\n",
    "    if method == 'fixed_point':\n",
    "        if inv_Sigma_Y is None:\n",
    "            raise ValueError(\"inv_Sigma_Y required for fixed_point method\")\n",
    "        B = inputs[:m*n].reshape(m, n)\n",
    "        Sigma_W = inputs[m*n:].reshape(m, m)\n",
    "        current_encoder = inputs.copy()\n",
    "        flag = True\n",
    "        for i in range(CONFIG.MAX_ITERS_MAP[method][n]):\n",
    "            if not flag:\n",
    "                scaled_inv_Sigma_Z = (inv(Sigma_Z) + 2 * lambda_ * np.eye(n)) / beta if lambda_ != 0 else inv(Sigma_Z) / beta\n",
    "                B = inv(np.eye(m) + A.T @ scaled_inv_Sigma_Z @ A) @ A.T @ scaled_inv_Sigma_Z\n",
    "                Sigma_W = inv(np.eye(m) + A.T @ scaled_inv_Sigma_Z @ A)\n",
    "                flag = True\n",
    "                B_diff_norm = np.linalg.norm(B - current_encoder[:m*n].reshape(m, n), 'fro')\n",
    "                Sigma_W_diff_norm = np.linalg.norm(Sigma_W - current_encoder[m*n:].reshape(m, m), 'fro')\n",
    "                if B_diff_norm <= tol_err and Sigma_W_diff_norm <= tol_err:\n",
    "                    break\n",
    "                current_encoder = np.concatenate((B.ravel(), Sigma_W.ravel()))\n",
    "            else:\n",
    "                inv_Sigma_W = inv(Sigma_W)\n",
    "                A = inv(inv_Sigma_Y + B.T @ inv_Sigma_W @ B) @ B.T @ inv_Sigma_W\n",
    "                Sigma_Z = inv(inv_Sigma_Y + B.T @ inv_Sigma_W @ B)\n",
    "                flag = False\n",
    "                if i == 0:\n",
    "                    current_decoder = np.concatenate((A.ravel(), Sigma_Z.ravel()))\n",
    "                else:\n",
    "                    A_diff_norm = np.linalg.norm(A - current_decoder[:n*m].reshape(n, m), 'fro')\n",
    "                    Sigma_Z_diff_norm = np.linalg.norm(Sigma_Z - current_decoder[n*m:].reshape(n, n), 'fro')\n",
    "                    if A_diff_norm <= tol_err and Sigma_Z_diff_norm <= tol_err:\n",
    "                        break\n",
    "                    current_decoder = np.concatenate((A.ravel(), Sigma_Z.ravel()))\n",
    "        if lambda_ == 0:\n",
    "            recon, _ = compute_vae_loss(A, B, Sigma_Z, Sigma_W, beta, n, m, Sigma_Y)\n",
    "        else:\n",
    "            recon, _ = compute_vae_loss(A, B, Sigma_Z, Sigma_W, beta, n, m, Sigma_Y, lambda_)\n",
    "    else:\n",
    "        A = torch.tensor(inputs[:n*m].reshape(n, m), dtype=torch.float64, requires_grad=True)\n",
    "        B = torch.tensor(inputs[n*m:n*m+m*n].reshape(m, n), dtype=torch.float64, requires_grad=True)\n",
    "        L_Z = torch.tensor(inputs[n*m+m*n:n*m+m*n+n*n].reshape(n, n), dtype=torch.float64, requires_grad=True)\n",
    "        L_W = torch.tensor(inputs[n*m+m*n+n*n:].reshape(m, m), dtype=torch.float64, requires_grad=True)\n",
    "        Sigma_Y_torch = torch.tensor(Sigma_Y, dtype=torch.float64)\n",
    "        optimizer = torch.optim.AdamW([A, B, L_Z, L_W], lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
    "        best_loss = float('inf')\n",
    "        best_metrics = None\n",
    "        for i in range(CONFIG.MAX_ITERS_MAP[method][n]):\n",
    "            optimizer.zero_grad()\n",
    "            Sigma_Z = L_Z @ L_Z.T + CONFIG.EPSILON * torch.eye(n, dtype=torch.float64)\n",
    "            Sigma_W = L_W @ L_W.T + CONFIG.EPSILON * torch.eye(m, dtype=torch.float64)\n",
    "            recon_err, loss = compute_vae_loss(A, B, Sigma_Z, Sigma_W, beta, n, m, Sigma_Y_torch, lambda_, is_torch=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([A, B], max_norm=CONFIG.GRAD_NORM_MAP[method][n]['AB'])\n",
    "            torch.nn.utils.clip_grad_norm_([L_Z, L_W], max_norm=CONFIG.GRAD_NORM_MAP[method][n]['LZ_LW'])\n",
    "            optimizer.step()\n",
    "            L_Z.data = torch.tril(L_Z.data)\n",
    "            L_W.data = torch.tril(L_W.data)\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_metrics = (\n",
    "                    A.detach().numpy().copy(),\n",
    "                    B.detach().numpy().copy(),\n",
    "                    Sigma_Z.detach().numpy().copy(),\n",
    "                    Sigma_W.detach().numpy().copy(),\n",
    "                    recon_err.item()\n",
    "                )\n",
    "        A, B, Sigma_Z, Sigma_W, recon = best_metrics\n",
    "    Sigma_X = B @ Sigma_Y @ B.T + Sigma_W\n",
    "    Sigma_XV = B @ Gamma @ Sigma_V\n",
    "    cov_XV = np.block([[Sigma_X, Sigma_XV], [Sigma_XV.T, Sigma_V]])\n",
    "    SAP, _ = compute_SAP(cov_XV, m, s)\n",
    "    Im, _ = compute_Im(cov_XV, m, s)\n",
    "    return float(recon), float(SAP), float(Im)\n",
    "    \n",
    "# =========================\n",
    "# Visualization\n",
    "# =========================\n",
    "def plot_boxplots(n, m, s):\n",
    "    \"\"\"Generate box plots for β-VAE metrics across all methods.\"\"\"\n",
    "    method_display_map = {\n",
    "        'fixed_point': 'Fixed Point',\n",
    "        'adamw': 'AdamW'\n",
    "    }\n",
    "    boxprops = dict(facecolor=\"#E6F3FF\", edgecolor=\"black\", linewidth=1)\n",
    "    positions = np.arange(len(CONFIG.BETA_ARR))\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(10, 12))\n",
    "    lambda_idx = np.where(CONFIG.LAMBDA_ARR == 0)[0][0]\n",
    "    for col, method in enumerate(CONFIG.METHODS):\n",
    "        recon_data = globals()[f\"{method}_recon_n{n}\"]\n",
    "        sap_data = globals()[f\"{method}_SAP_n{n}\"]\n",
    "        im_data = globals()[f\"{method}_Im_n{n}\"]\n",
    "        method_display = method_display_map[method]\n",
    "        data_dict = {\n",
    "            'recon': [recon_data[i, lambda_idx, :] for i in range(len(CONFIG.BETA_ARR))],\n",
    "            'sap': [sap_data[i, lambda_idx, :] for i in range(len(CONFIG.BETA_ARR))],\n",
    "            'im': [im_data[i, lambda_idx, :] for i in range(len(CONFIG.BETA_ARR))]\n",
    "        }\n",
    "        for row, (data_key, ylabel, title) in enumerate([\n",
    "            ('recon', 'Reconstruction Error', 'Reconstruction Error'),\n",
    "            ('sap', 'SAP Score', 'SAP Score'),\n",
    "            ('im', r'$I_m$ Score', r'$I_m$ Score')\n",
    "        ]):\n",
    "            ax = axes[row, col]\n",
    "            ax.boxplot(\n",
    "                data_dict[data_key], positions=positions, widths=0.4, showmeans=True,\n",
    "                patch_artist=True, boxprops=boxprops\n",
    "            )\n",
    "            ax.set_ylabel(ylabel)\n",
    "            if row == 0:\n",
    "                ax.set_title(method_display, fontsize=13)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xticks(positions)\n",
    "            if row == 2:\n",
    "                ax.set_xticklabels([f\"{b}\" for b in CONFIG.BETA_ARR], fontsize=12)\n",
    "            else:\n",
    "                ax.set_xticklabels([])\n",
    "            if row == 2:\n",
    "                ax.set_xlabel(r\"$\\beta$\", fontsize=12)\n",
    "            ax.legend(\n",
    "                handles=[\n",
    "                    plt.Line2D([0], [0], color=\"orange\", linewidth=2, label=\"Median\"),\n",
    "                    plt.Line2D([0], [0], marker=\"^\", color=\"green\", linestyle=\"None\", markersize=6, label=\"Mean\")\n",
    "                ],\n",
    "                loc=\"best\", fontsize=8, frameon=True, edgecolor=\"black\", framealpha=0.9\n",
    "            )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG.SAVE_PATH, f\"boxplots_betavae_n{n}_m{m}_s{s}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "def plot_heatmap(n, m, s):\n",
    "    \"\"\"Generate heatmap plots for λβ-VAE metrics across all methods.\"\"\"\n",
    "    method_display_map = {\n",
    "        'fixed_point': 'Fixed Point',\n",
    "        'adamw': 'AdamW'\n",
    "    }\n",
    "    recon_cmap = LinearSegmentedColormap.from_list('custom_red', ['#FFE6E6', '#A93226'])\n",
    "    blue_cmap = LinearSegmentedColormap.from_list('custom_blue', ['#E6F3FF', '#4682B4'])\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(10, 9))\n",
    "    beta_idx = np.where(CONFIG.BETA_ARR == 1)[0][0]\n",
    "    lambda_idx = np.where(CONFIG.LAMBDA_ARR == 0)[0][0]\n",
    "    for col, method in enumerate(CONFIG.METHODS):\n",
    "        metrics = {\n",
    "            'recon': np.nanmean(globals()[f'{method}_recon_n{n}'], axis=-1).T,\n",
    "            'sap': np.nanmean(globals()[f'{method}_SAP_n{n}'], axis=-1).T,\n",
    "            'im': np.nanmean(globals()[f'{method}_Im_n{n}'], axis=-1).T\n",
    "        }\n",
    "        method_display = method_display_map[method]\n",
    "        for row, (mean, title, cmap) in enumerate([\n",
    "            (metrics['recon'], 'Mean Reconstruction Error', recon_cmap),\n",
    "            (metrics['sap'], 'Mean SAP Score', blue_cmap),\n",
    "            (metrics['im'], 'Mean $I_m$ Score', blue_cmap)\n",
    "        ]):\n",
    "            ax = axes[row, col]\n",
    "            im = ax.imshow(mean, cmap=cmap, aspect='auto')\n",
    "            if row == 0:\n",
    "                ax.set_title(f\"{method_display}\", fontsize=13)\n",
    "            ax.set_yticks(np.arange(len(CONFIG.LAMBDA_ARR)))\n",
    "            ax.set_yticklabels([f'{l}' for l in CONFIG.LAMBDA_ARR] if col == 0 else [])\n",
    "            ax.set_xticks(np.arange(len(CONFIG.BETA_ARR)))\n",
    "            ax.set_xticklabels([f'{b}' for b in CONFIG.BETA_ARR] if row == 2 else [])\n",
    "            ax.grid(False)\n",
    "            plt.colorbar(im, ax=ax, label=title)\n",
    "            for i, j in np.ndindex(len(CONFIG.LAMBDA_ARR), len(CONFIG.BETA_ARR)):\n",
    "                ax.text(j, i, f'{mean[i, j]:.3f}', ha='center', va='center', color='black', fontsize=12)\n",
    "                if i == lambda_idx and j == beta_idx:\n",
    "                    ax.add_patch(Rectangle((j-0.5, i-0.5), 1, 1, linewidth=2, edgecolor='red', facecolor='none'))\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(r'$\\lambda$')\n",
    "            if row == 2:\n",
    "                ax.set_xlabel(r'$\\beta$')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG.SAVE_PATH, f'heatmap_lambda_betavae_n{n}_m{m}_s{s}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
    "    for n in CONFIG.N_VALUES:\n",
    "        m, s = CONFIG.MS_MAP[n]\n",
    "        print(colored(f'\\nn = {n}, m = {m}, s = {s}:', 'blue', attrs=['bold']))\n",
    "        # Initialize metric arrays\n",
    "        shape = (len(CONFIG.BETA_ARR), len(CONFIG.LAMBDA_ARR), CONFIG.NUM_SOLS)\n",
    "        for method in CONFIG.METHODS:\n",
    "            globals()[f'{method}_recon_n{n}'] = np.full(shape, np.nan)\n",
    "            globals()[f'{method}_SAP_n{n}'] = np.full(shape, np.nan)\n",
    "            globals()[f'{method}_Im_n{n}'] = np.full(shape, np.nan)\n",
    "        # Generate covariance matrices\n",
    "        Sigma_V = generate_diag_cov(s)\n",
    "        np.savez(os.path.join(CONFIG.SAVE_PATH, f'Sigma_V_n{n}_m{m}_s{s}.npz'), Sigma_V=Sigma_V)\n",
    "        Gamma = np.random.randn(n, s)\n",
    "        Sigma_Y = Gamma @ Sigma_V @ Gamma.T + 0.05 * np.eye(n)\n",
    "        inv_Sigma_Y = inv(Sigma_Y)\n",
    "        np.savez(os.path.join(CONFIG.SAVE_PATH, f'Gamma_Sigma_Y_n{n}_m{m}_s{s}.npz'), Gamma=Gamma, Sigma_Y=Sigma_Y)\n",
    "        # Initialize parameters\n",
    "        initial_params = {\n",
    "            'fixed_point': [initialize_inputs(n, m, 'fixed_point') for _ in range(CONFIG.NUM_SOLS)],\n",
    "            'adamw': [initialize_inputs(n, m, 'adamw', seed=42 + i) for i in range(CONFIG.NUM_SOLS)]\n",
    "        }\n",
    "        # Run optimization for each method sequentially\n",
    "        for method in CONFIG.METHODS:\n",
    "            print(colored(f'\\nRunning {method.upper()} method...', attrs=['bold']))\n",
    "            for b_idx, beta in enumerate(CONFIG.BETA_ARR):\n",
    "                for l_idx, lam in enumerate(CONFIG.LAMBDA_ARR):\n",
    "                    results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "                        delayed(optimize)(\n",
    "                            inputs, beta, lam, n, m, s, Sigma_Y, Gamma, Sigma_V, method,\n",
    "                            tol_err=CONFIG.TOL_ERR_MAP[n] if method == 'fixed_point' else None,\n",
    "                            inv_Sigma_Y=inv_Sigma_Y if method == 'fixed_point' else None\n",
    "                        ) for inputs in tqdm(initial_params[method], desc=f\"β={beta}, λ={lam}\", ncols=100)\n",
    "                    )\n",
    "                    recon_vals, SAP_vals, Im_vals = zip(*results)\n",
    "                    globals()[f'{method}_recon_n{n}'][b_idx, l_idx, :len(recon_vals)] = recon_vals\n",
    "                    globals()[f'{method}_SAP_n{n}'][b_idx, l_idx, :len(SAP_vals)] = SAP_vals\n",
    "                    globals()[f'{method}_Im_n{n}'][b_idx, l_idx, :len(Im_vals)] = Im_vals\n",
    "        plot_boxplots(n, m, s)\n",
    "        plot_heatmap(n, m, s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
